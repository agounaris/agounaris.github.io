[{"content":"This project examines the predictive capabilities of the ARIMA model for time series analysis. To explore the capabilities of the statsmodels library, the Python programming language will be used with jupyter notebook being the runtime environment. Along with the prediction analysis, there will be a demonstration of data retrieval using the yahoo web api, time series transformation, manual and automatic stationarity checks, benchmarking the model versus a naive benchmark, multiple days prediction and multiple symbol prediction mean error checks. The Python code in this project can be used to create a reusable module for predicting any combination of symbols with given date ranges.\nPrerequisites NEEDED BACKGROUND The reader must be able to understand econometrics processes and be able to read Python source code. The Python ecosystem offers a great set of tools for scientific computing the libraries can be installed with the pip package management tool. The basic library I will use for this experiment is the statsmodels, a very powerful statistics library offering many statistics functions http://statsmodels.sourceforge.net/devel/index.html#table-of-contents . THE ARIMA MODEL In econometrics and time series analysis the ARIMA model, an acronym for AutoRegressive Integrated Moving Average, is a model that allows us to better understand the time series data and also predict points in the future. The model is represented as ARIMA(p, d, q) when p, d, q are non negative integers and they represent the number of passes on each individual process of the model.\np: corresponds to the “AR” part of the model. An AR(1) is called a first-order auto-regressive model for Y and it defines simple regression with an independent variable being a lagged value of Y(ex Y+1).\nd: corresponds to the “I” part of the model. An I(1) is called a first-order integrated model and it defines the number of the differentiations of the time series in order to make them stationary. We define stationary series as the series whose mean, variance and autocorrelation are constant over time. Stationary series are easier to predict and stationarity can be approximated by differentiating the data points.\nq: corresponds to the “MA” part of the model. An MA(1) is called a first-order moving average model and it defined the lagged forecast errors of the prediction equation.\nDIAGNOSTICS STATIONARITY We can test a time series for stationarity using the “Augmented Dickey - Fuller” test. This is a test which executes a null hypothesis testing that there is a unit-root with the alternative hypothesis that there is no unit root. A unit root is a random trend in time series and it adds a negative effect on any attempt of prediction. If the p-value of the ad-fuller test is above a critical limit then we cannot reject the null hypothesis and the presence of a unit root. Statsmodels offers a function to perform the ad-fuller test on a time series which will be used in this experiment.\nDURBIN WATSON TEST The Durbin-Watsos test is a test statistic which reports the presence of autocorrelation in the time series. As autocorrelation we define the error of a period, transferred to another period. For example the underestimation of some profit for period A causes an underestimation of a profit for period B. This test can returns a value between 0 and 4 with 2 being no autocorrelation and the rest of the values defining the presence of positive autocorrelation (0\u0026lt;2) and negative autocorrelation (2\u0026lt;4).\nCode Implementation In order to showcase this process as good as possible, I will describe the code statements in a procedural way.\nNeeded modules import The following statements load all the needed libraries for the execution of the statements. Note the rcParams dictionary. This is a way to customize how the matplotlib plots will be rendered.\n%matplotlib inline import os.path import datetime import pandas as pd import numpy as np import matplotlib.pyplot as plt import pandas_datareader.data as web import statsmodels.api as sm import statsmodels.tsa.api as smt from statsmodels.tsa.arima_model import ARIMA, ARIMAResults from statsmodels.graphics.tsaplots import plot_acf, plot_pacf from matplotlib.pylab import rcParams rcParams[\u0026#39;figure.figsize\u0026#39;] = 15, 6 plt.style.use(\u0026#39;ggplot\u0026#39;) Declaring the data space This is the data we will use to train and test the model. Note that the given date range is not going to work for all stock symbols. The user needs to be aware of the date a specific symbol started to trade otherwise the model will throw errors about missing data.\nsymbol = \u0026#39;MSFT\u0026#39; source = \u0026#39;yahoo\u0026#39; start_date = \u0026#39;2008-01-04\u0026#39; end_date = \u0026#39;2017-04-06\u0026#39; predict_days = 2 filename = \u0026#39;{}_{}_to_{}.csv\u0026#39;.format(symbol, start_date, end_date) Retrieving the data To retrieve the data I am using the pandas datareader. Remember the \u0026ldquo;source\u0026rdquo; variable from the input declaration step? This is the actual service from where the data will be fetched. On the first run the data are fethced from the remote service and they are stored on a given filename using the csv file format. If you re-run this step the data will be loaded from the saved file. To achieve a naming consistency the filename consists of the symbol name and the date range as declared on the input declaration step.\nstart = datetime.datetime.strptime(start_date, \u0026#34;%Y-%m-%d\u0026#34;) end = datetime.datetime.strptime(end_date, \u0026#34;%Y-%m-%d\u0026#34;) if os.path.isfile(filename): data = pd.read_csv(filename, parse_dates=True, index_col=0) else: data = web.DataReader(symbol, source, start, end) data.to_csv(filename) data.tail(5) Open High Low Close Volume Adj Close Date 2017-03-31 65.650002 66.190002 65.449997 65.860001 21001100 65.860001 2017-04-03 65.809998 65.940002 65.190002 65.550003 20352800 65.550003 2017-04-04 65.389999 65.809998 65.279999 65.730003 12981200 65.730003 2017-04-05 66.300003 66.349998 65.440002 65.559998 21381100 65.559998 2017-04-06 65.599998 66.059998 65.480003 65.730003 18070500 65.730003 Transforming Data The data from the api do not have available values for the dates the market is closed. This will cause errors on the processing of the data. The ffill method will be userd to add those missing data with the value of the previous available date.\nOn this section I will compile the train and test data. This will be two sliced datasets out of the original time series. During this process I am also filtering out the columns I don\u0026rsquo;t need keeping just the Close price of the stock, which will be the values up the model will be build.\noriginal_data = data.fillna(method=\u0026#39;ffill\u0026#39;) ran = pd.date_range(start_date, end_date, freq = \u0026#39;D\u0026#39;) original_data = pd.Series(original_data[\u0026#39;Close\u0026#39;], index = ran) plt.plot(original_data) plt.show() original_data = original_data.fillna(method=\u0026#39;ffill\u0026#39;) split = len(original_data) - predict_days train_data, prediction_data = original_data[0:split], original_data[split:] train_data.tail(5) 2017-03-31 65.860001 2017-04-01 65.860001 2017-04-02 65.860001 2017-04-03 65.550003 2017-04-04 65.730003 Freq: D, Name: Close, dtype: float64 Diagnostics Stationarity For the model to be build we need to check the series for stationarity. The code below renders the autocorrelation and partial autocorrelation charts.\n# rendering such a large time series is compute intensive so I\u0026#39;m breaking it up didive_large_series_by = 10 fig, axes = plt.subplots(1, 2, figsize=(15,4)) fig = plot_acf(train_data, lags = abs(train_data.shape[0]/didive_large_series_by), ax=axes[0]) fig = plot_pacf(train_data, lags = abs(train_data.shape[0]/didive_large_series_by), ax=axes[1]) The autocorrelation plot indicates that the time series are not stationary because the values are not reduced at a significant pace. The partial autocorrelation graph above indicates that the value on lag 1 is the only one which is significantly different from 0, so a model AR(1) should be enough.\nsymbol_diff = train_data - train_data.shift() symbol_diff = symbol_diff.dropna() symbol_diff.head(4) 2008-01-05 0.00 2008-01-06 0.00 2008-01-07 0.23 2008-01-08 -1.16 Freq: D, Name: Close, dtype: float64 plt.plot(symbol_diff) plt.title(\u0026#39;First Difference Time Series Plot\u0026#39;) plt.show() These are the manual steps that can be followed in order to check a signal for stationarity and transform it to be usable for the ARMA models. Since this process is quite manual and there needs to be way to let the program decide whether a time series is stationary or not I am going to use the Ad-fuller test on the next step.\nAd-fuller test The process above showed how you can check for stationarity and manually transform the time series. With the ad-fuller test we have a process to check for stationarity which, computetionally, is more convenient.\nfrom collections import namedtuple ADF = namedtuple(\u0026#39;ADF\u0026#39;, \u0026#39;adf pvalue usedlag nobs critical icbest\u0026#39;) stationarity_results = ADF(*smt.adfuller(train_data))._asdict() significance_level = 0.01 if (stationarity_results[\u0026#39;pvalue\u0026#39;] \u0026gt; significance_level): message = \u0026#39;For p-value={:0.4f}, the time series are probably non-stationary\u0026#39; print(message.format(stationarity_results[\u0026#39;pvalue\u0026#39;])) else: message = \u0026#39;For p-value={:0.4f}, the time series are probably stationary\u0026#39; print(message.format(stationarity_results[\u0026#39;pvalue\u0026#39;])) print(stationarity_results[\u0026#39;critical\u0026#39;]) For p-value=0.9911, the time series are probably non-stationary {'5%': -2.862399816394662, '1%': -3.4322959703836289, '10%': -2.5672276970758641} Building the model I will pick an ARIMA(1,1,1) because of non-stationary time series. I\u0026rsquo;ve noticed that you cannot use any kind of combination on ARIMA(p, d, q). For example the (1,2,1) fails to converge while the (3,2,1) works better and has smaller error rates. An explanation of ARIMA(1,1,1) could be the following. We need a model which will try to explain data points with their mean, variance and autocorrelation not being constant, by fixing them with differenciation and exponential smoothing. Practically the AR(1) fixes the positive autocorrelation while and MA(1) fixes the negative autocorrelation. Autocorrelation appears a lot in this context. There are many available explanations on what autocorrelation is. Practically when you analyze time series you don\u0026rsquo;t want the attribute of a specifit data point to affect the attributes of another data point in the future. For example the fact that this months sales are low, should not affect the sales of the next month. You don\u0026rsquo;t want your data points to be influenced by their own older values. p and q also affect the coefficients of the output. The Nth order creates N coefficients. order = (1, 0, 1) # if the series are stationary, there is no need for an integrated order if stationarity_results[\u0026#39;pvalue\u0026#39;] \u0026gt; 0.01: order = (1, 1, 1) mod = ARIMA(train_data, order = order, freq = \u0026#39;D\u0026#39;) results = mod.fit() print(results.summary()) print(\u0026#39;DW test is {}\u0026#39;.format(sm.stats.durbin_watson(results.resid.values))) ARIMA Model Results ============================================================================== Dep. Variable: D.Close No. Observations: 3378 Model: ARIMA(1, 1, 1) Log Likelihood -2289.348 Method: css-mle S.D. of innovations 0.477 Date: Thu, 04 May 2017 AIC 4586.697 Time: 16:21:40 BIC 4611.197 Sample: 01-05-2008 HQIC 4595.456 - 04-04-2017 ================================================================================= coef std err z P\u0026gt;|z| [0.025 0.975] --------------------------------------------------------------------------------- const 0.0093 0.007 1.405 0.160 -0.004 0.022 ar.L1.D.Close 0.8490 0.059 14.483 0.000 0.734 0.964 ma.L1.D.Close -0.8781 0.053 -16.637 0.000 -0.982 -0.775 Roots ============================================================================= Real Imaginary Modulus Frequency ----------------------------------------------------------------------------- AR.1 1.1779 +0.0000j 1.1779 0.0000 MA.1 1.1388 +0.0000j 1.1388 0.0000 ----------------------------------------------------------------------------- DW test is 2.01217370914 The first thing we need to mention on the summary above is that the true parameters(coef) exist in the 95% confidence interval. The durbin watson test is close to 2 which indicates the lack of autocorrelation.\nprediction = results.predict(prediction_data.index[0], prediction_data.index[-1], typ=\u0026#39;levels\u0026#39;) prediction.tail(predict_days) 2017-04-05 65.730335 2017-04-06 65.732022 Freq: D, dtype: float64 fig, ax = plt.subplots(figsize=(12, 8)) plt.title(\u0026#34;Actual Close price Vs. Forecasted Values\u0026#34;) ax = original_data.ix[len(original_data)-predict_days*10:].plot(ax=ax) fig = results.plot_predict(prediction_data.index[0], prediction_data.index[-1], dynamic=True, ax=ax, plot_insample=False) legend = ax.legend(loc=\u0026#39;upper left\u0026#39;) The graph above shows the predicted values as well and a confidence interval indicating that we can be 95% sure that the price will be between 64.5 and 67. This graph and this confidence interval is rendered for the MSFT symbol using the date range from 2008-01-04 to 2017-04-04 as train data and predicting the dates 2017-04-05 and 2017-04-06.\nOn time series the perfomance of the model is quantified by the mean forecast error which I am calculating on the next step.\nmean_forecast_error = original_data.ix[-predict_days:].sub(prediction).mean() print(\u0026#39;Mean forecast error is {:.2%}\u0026#39;.format(abs(mean_forecast_error))) Mean forecast error is 8.62% Benchmarking I\u0026rsquo;m going to check the performance of my model towards a naive prediction. Naive prediction is simply the last day\u0026rsquo;s value as a forecast for the next day. In python you can simulate this by shifting the prices of the original data by one and compare them with the predicted values as bellow.\nnaive_prediction = original_data.tail(predict_days+1).shift(1).tail(predict_days) percentage_change = abs((prediction/naive_prediction-1) *100) df = pd.concat([naive_prediction, prediction, percentage_change], axis=1) df.columns = [\u0026#39;Original\u0026#39;, \u0026#39;Predicted\u0026#39;, \u0026#39;% Prediction Diff\u0026#39;] df Original Predicted % Prediction Diff 2017-04-05 65.730003 65.730335 0.000505 2017-04-06 65.559998 65.732022 0.262391 Compine information Lets try to check the mean errors for a range of prediction days. To do this I use the model I\u0026rsquo;ve created above and with every iteration I will be calculating again the train and the test data based on the number of the given prediction days.\nmean_errors = [] for number_of_days in range(1, 20): split = len(original_data) - number_of_days train_data, prediction_data = original_data[0:split], original_data[split:] mod = ARIMA(train_data, order = (1, 1, 1), freq = \u0026#39;D\u0026#39;) results = mod.fit(disp=0) prediction = results.predict(prediction_data.index[0], prediction_data.index[-1], typ=\u0026#39;levels\u0026#39;) original_data_sample = original_data.ix[-number_of_days:] mean_errors.append(abs(original_data_sample.sub(prediction).mean())) plt.plot(mean_errors) plt.show() As we notice there is a good prediction rate up to the first 6 days with a mean error of around 20%. After that the error explodes and it even exceeds the 100% mark on the 15 days.\nOne more interesting experiment is to run the same model for a number of different stock symbols and check how it performs on different signals. I will focus on similar industry symbols.\nsymbols = [\u0026#39;MSFT\u0026#39;, \u0026#39;NVDA\u0026#39;, \u0026#39;CSCO\u0026#39;, \u0026#39;ORCL\u0026#39;, \u0026#39;SYMC\u0026#39;, \u0026#39;INTC\u0026#39;, \u0026#39;EBAY\u0026#39;, \u0026#39;YHOO\u0026#39;] mean_errors = [] for symbol in symbols: # this can be improved with multiprocessing or async calls data = web.DataReader(symbol, source, start, end) original_data = data.fillna(method=\u0026#39;ffill\u0026#39;) ran = pd.date_range(start_date, end_date, freq = \u0026#39;D\u0026#39;) original_data = pd.Series(original_data[\u0026#39;Close\u0026#39;], index = ran) original_data = original_data.fillna(method=\u0026#39;ffill\u0026#39;) split = len(original_data) - predict_days train_data, prediction_data = original_data[0:split], original_data[split:] mod = ARIMA(train_data, order = (1, 1, 1), freq = \u0026#39;D\u0026#39;) results = mod.fit(disp=0) prediction = results.predict(prediction_data.index[0], prediction_data.index[-1], typ=\u0026#39;levels\u0026#39;) original_data_sample = original_data.ix[-number_of_days:] mean_errors.append(abs(original_data_sample.sub(prediction).mean())) N = len(mean_errors) x = range(N) width = 1/1.5 plt.bar(x, mean_errors, width, color=\u0026#34;blue\u0026#34;) plt.xticks(x, symbols) plt.show() The experiment above indicates that for the most symbols there was a very good prediction mean error of less than 20%. The model failed to predict NVIDIA complitelly and was less performant for CISCO and ORACLE.\nConclusions With this exercise I have tested the predictive capabilities of the ARIMA group of models. I have also investigated the performance of my models towards multiple input days and the performance on multiple input symbols for the same date range.\nARIMA proved to be quite performant in most occasions. The mean forecast error for predicting two days for the microsoft stock price proved to be just 0.26% different than the naive benchmark for the same days.\nThere are some cases where the model did not perform well. Henry and Mizon wrote an interesting paper on unpredictability of the econometric modeling and I quote “Unpredictability arises from intrinsic stochastic variation, unexpected instances of outliers, and unanticipated extrinsic shifts of distributions.”. A simple explanation of this could summarized as follows. An econometric model can fail because of internal unknown processes, unexpected failures which affect the train data and changes on external distributions such as the market, economy or governmental policy.\nAnother interesting project, would be to expand this time series analysis and use multiple models for the same prediction period and pick the one which works the best for the specific underlying target. In modern computing systems this process can be spanned across multiple machines and predict multiple combinations of symbols and date ranges in parallel.\nDetailed instructions on how to run this notebook Probably the easiest solution for non python experts is the anaconda distribution https://www.continuum.io/downloads. It contains a great set of tool for scientific computing out of the box. The full list of available modules is here https://docs.continuum.io/anaconda/pkg-docs. If you will go with this solution jump to step 7. Check your python distribution with python -V. It needs to be at least 2.7 and have the pip package manager installed. Check the pip version with pip -V If case you do not have pip installed check out the instructions here https://pip.pypa.io/en/stable/installing/ Install virtualenv https://virtualenv.pypa.io/en/stable/installation/. Virtualenv is a tool which allows you to have separate python groups of modules in order to keep the global Python installation clean and avoid module versions conflicts. Create a virtual env with the command mkvirtualenv arimamodel and verify you are using this environment with the command workon arimamodel Update pip with pip install -U pip Install all the needed modules with pip install numpy==1.12.1 statsmodels==0.8.0 matplotlib==2.0.0 jupyter==1.0.0 pandas-datareader==0.3.0.post0 Navigate to the directory where this .ipynb file is and run the command jupyter notebook. You should be redirected to the browser and you should be able to select this specific notebook. Make sure you execute each cell to make all the statements available to the code flow. You can run each cell either by selecting it and click on Cell-\u0026gt;Run Cell from the top menu or by selecting it and hit the keys Crtl+Enter. Works cited Ayodele A. Adebiyi, Aderemi O. Adewumi, Charles K. Ayo, “Stock Price Prediction Using the ARIMA Model”, UKSim-AMSS 16th International Conference on Computer Modelling and Simulation 2014\nDavid F. Hendry, Grayham E. Mizon, “Unpredictability in Economic Analysis, Econometric Modeling and Forecasting” University of Oxford 2013\nMichael Wagner, “Forecasting Daily Demand in Cash Supply Chains”, American Journal of Economics and Business Administration 2 (4): 377-383, 2010 ISSN 1945-5488, 2010\nJavier Contreras, Rosario Espínola, Francisco J. Nogales, Antonio J. Conejo, “ARIMA Models to Predict Next-Day Electricity Prices”, IEEE Transactions on power systems, vol 18, no 3, Aug. 2003\nIMPORTANT NOTE This notebook exists for informational purposes only. This analysis does not contain quality predictions. A production system has to cross-validate the findings and predict a greater amount of assets in order to create valid trading strategies. There are no guarantees for accuracy or completeness and this guide can change at any time.\n","permalink":"/2017/05/01/2017-05-01-stock-symbol-time-series-analysis-and-prediction-with-arima-model/","summary":"This project examines the predictive capabilities of the ARIMA model for time series analysis. To explore the capabilities of the statsmodels library, the Python programming language will be used with jupyter notebook being the runtime environment. Along with the prediction analysis, there will be a demonstration of data retrieval using the yahoo web api, time series transformation, manual and automatic stationarity checks, benchmarking the model versus a naive benchmark, multiple days prediction and multiple symbol prediction mean error checks.","title":"Stock symbol time series analysis and prediction with ARIMA model"},{"content":"Today Greece is again all over the news because of the Eurogroup and the new deal the Greek governemnt tries to achieve through its representatives. Why not check out which website has a title about Greece on it\u0026rsquo;s front page as fast as possible.\nPython 3.2 introduced concurrent futures which is actually a simple interface for asynchronous parallel tasks. Lets check it out on a very simple example.\nWe will crawl 5 web sites, some of them will not contain English characters and I selected them just to check the difference. I will use lxml and requests which are easily installed with pip (pip3 in my case :)).\nThe simple iteration through the urls\n{% highlight python startinline=true %} from lxml import html import requests import sys\nurls = [ \u0026lsquo;http://www.theguardian.com/uk\u0026rsquo;, \u0026lsquo;http://www.bbc.co.uk\u0026rsquo;, \u0026lsquo;http://www.in.gr\u0026rsquo;, \u0026lsquo;http://www.ethnos.gr\u0026rsquo;, \u0026lsquo;https://uk.yahoo.com\u0026rsquo; ]\ndef parse(urls): for url in urls: page = requests.get(url) print(page.text.find(sys.argv[1]))\nparse(urls) {% endhighlight %}\n![simple loop iteration]({{ site.url }}/assets/media/single.png)\nThe asynchronous concurrent approach\n{% highlight php startinline=true %} import concurrent.futures import urllib.request from lxml import html import requests import sys\nurls = [ \u0026lsquo;http://www.theguardian.com/uk\u0026rsquo;, \u0026lsquo;http://www.bbc.co.uk\u0026rsquo;, \u0026lsquo;http://www.in.gr\u0026rsquo;, \u0026lsquo;http://www.ethnos.gr\u0026rsquo;, \u0026lsquo;https://uk.yahoo.com\u0026rsquo; ]\ndef parse(url): page = requests.get(url) print(page.text.find(sys.argv[1]))\nwith concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\nfuture_to_url = {executor.submit(parse, url): url for url in urls} for future in concurrent.futures.as_completed(future_to_url): url = future_to_url[future] {% endhighlight %}\n![parallel iteration]({{ site.url }}/assets/media/multi.png)\nThe execution of all urls was in average 2 times faster on the parallel approach. As you might noticed just because of the asynchronous nature the result of the second url comes as third on the parallel script.\nYou can very easily replace the content of parse function to process your own set of data :)\nCheers\n","permalink":"/2015/02/28/2015-02-16-asynchronous-python3-parallel-tasks/","summary":"Today Greece is again all over the news because of the Eurogroup and the new deal the Greek governemnt tries to achieve through its representatives. Why not check out which website has a title about Greece on it\u0026rsquo;s front page as fast as possible.\nPython 3.2 introduced concurrent futures which is actually a simple interface for asynchronous parallel tasks. Lets check it out on a very simple example.\nWe will crawl 5 web sites, some of them will not contain English characters and I selected them just to check the difference.","title":"Asynchronous tasks with python3 concurrent futures"},{"content":"After a year working with elasticsearch I decided to write about it mainly as a self reminder of what I liked what I did not and the choices I made in the process. Everything is a personal opinion and someone might choose to disagree but it’s cool :) . All this experience was a part of creating a search implementation for an e-commerce application which in the process along with a mysql full text search was contributed back to sylius, the “base” for our solution.\nFirst things first, why elasticsearch compared to other search engines?\nWhat I loved in elasticsearch is that it’s “modern”. For me the rest api it exposes it’s just the way I though a search engine for a web app will be and elasticsearch is a cluster by default compared to other search engines. Also it can work as a document database or caching layer but my approach towards caching is that you must be available at any given time to rebuild it so I wouldn’t like to use it for persistence.\nElasticsearch is massive!! And it changes fast! Documentation is good enough but I would like a bit more details on day-to-day tasks. What confused me initially was the fact that all the examples provided in the documentation must be wrapped in a “query” clause.\nSetting up ES: Starting with elasticsearch is really easy. I prefer downloading the zip containing the executables and if you have mac I like homebrew. When we started with es I wanted to automate the setup of a simple cluster and I choose vagrant with puppet for this. The first version was just setting up a 2 nodes cluster in VMs just for the development. Later on I’ve build a more generic environment using the latest practices.\nLibraries: Our application is a symfony one. That said I just started using the FosElasticaBundle which is a wrapper for the elastica library. What it provides you is yml configuration for your indexes and mechanisms like update/delete/insert listeners for your indexed data.\nFosElastica work amazingly if you have a small application or a model that is not so complicated. When I say complicated I mostly mean relations between objects. Our application is an e-commerce one and our model is quite challenging to index. I started with the default settings and soon enough I realised that the level of nested objects will do the querying quite complicated. By using a mechanism called property accessor I took the decision to get all what I need on the indexing time and save them in a single indexing level were attributes will be either scalar or arrays. This helped me to keep the query code really simple and straightforward. But this didn’t end there….\nNow imagine an object which has options and attributes…By using listeners when an option changes you have to go and search for all the objects which includes it and re-index them. Think that maybe you have 3 or 4 options changing all the time. Suddenly indexing with listeners becomes totally inefficient. This is where we decided to drop FosElastica for our app and use just the elastica lib for quering ES and build our own custom indexer. One thing that makes indexing a bit slow it’s also the ORM. Doctrine in this case. We dumped that as well. After some series of refactoring and optimisations and by using raw mysql queries we managed to index the whole catalog for multiple regions in under 9 seconds. We accepted that the indexer will not have fresh data at any given time but we managed to do the re-indexing so fast that will be hardly noticeable by the end user. Not mention that we always go to the database when it comes to add something to the cart.\nWorkflow: I always have a rest client open to have direct access to the indexed data. I tried out several chrome plugins including postman but I ended up using the HEAD plugin of the elasticsearch itself. It really has everything you need to execute queries and check the status of your indexes.\nI started developing the app trying to query data from the elastica library itself. This proven to be slightly wrong…elastica is a nice object oriented library around creating arrays that afterwards are json-encoded before the query. This interface can be confusing to someone which is not able to compile a complicated query properly, like I was 1 year ago. What I like doing now it to start always with the raw json query, after I get the result I want it’s dead obvious how to transfer the query to the library.\nA must for an e-commerce application is the faceted result set. Or in elasticsearch terms the aggregations. I was developing the app using the facets and it was that time when ES took the decision to deprecate facets for the favour of aggregations. Is was not a massive change to do in the code but it added a level of frustration, since you want to deliver something in time. A small issue also occurred with the default behaviour of setFilter which now points to setPostFilter and affects the counts of the aggregations. Again I had to adapt my code for this change since I wanted the aggregations to be applied to the unfiltered result set but now ES is doing this for me.\nUsage: We use elasticsearch for search, app logging and analytics. For search we index our catalog, store infos, web content etc and there is a smart mechanism to filter those different result sets and load separate views and represent them on the same result set. We have an ObjectPresentationHandler which abstract a bit the way things are shown on the gui.\nFor app logging we use monolog and we post data through channels mostly for payments and internal app procedures. MonologBundle initially was not supporting ES engine and because we needed it I’ve contributed it back to the bundle.\nFor analytics the idea is to throw in ES all the raw data within a specific date range and use kibana afterwards to extract useful information. We use it mostly for sales analytics, order states etc.\nElasticsearch is a great tool, easy to setup with good documentation that fits really well in a lot of use cases!!\nCheers, Argyris\n","permalink":"/2015/02/28/2015-02-28-1-year-developing-with-es/","summary":"After a year working with elasticsearch I decided to write about it mainly as a self reminder of what I liked what I did not and the choices I made in the process. Everything is a personal opinion and someone might choose to disagree but it’s cool :) . All this experience was a part of creating a search implementation for an e-commerce application which in the process along with a mysql full text search was contributed back to sylius, the “base” for our solution.","title":"One year developing with Elasticsearch"},{"content":"Recently I came upon a problem where the passwords of the current user base are hashed in md5 and we had to migrate them to sha. Checking around the web how others have solved this didn\u0026rsquo;t help a lot. Asking your user to login to change the password or double encode the md5 one didn\u0026rsquo;t sound like clever solutions.\nThe main authentication process in symfony is happening in DaoAuthenticationProvider.\n{% highlight php startinline=true %} if (!$this-\u0026gt;encoderFactory-\u0026gt;getEncoder($user)-\u0026gt;isPasswordValid($user-\u0026gt;getPassword(), $presentedPassword, $user-\u0026gt;getSalt())) { throw new BadCredentialsException(\u0026lsquo;The presented password is invalid.\u0026rsquo;); } {% endhighlight %}\nWhat I end up doing is to actually leave this as a final password check but prior to that authenticate the user once with the md5 and generate the sha with the given password. On the same I clear the legacy md5 field in order to know which users are actually updated and prevent the check of happening again in the future.\n{% highlight php startinline=true %} if (\u0026quot;\u0026quot; !== ($legacyPassword = $user-\u0026gt;getLegacyPassword())) { if ($legacyPassword === hash(\u0026lsquo;md5\u0026rsquo;, $presentedPassword)) { $user-\u0026gt;setPlainPassword($presentedPassword); $user-\u0026gt;setLegacyPassword(null); $this-\u0026gt;userManager-\u0026gt;updateUser($user); } }\nif (!$this-\u0026gt;encoderFactory-\u0026gt;getEncoder($user)-\u0026gt;isPasswordValid($user-\u0026gt;getPassword(), $presentedPassword, $user-\u0026gt;getSalt())) { throw new BadCredentialsException(\u0026lsquo;The presented password is invalid.\u0026rsquo;); } {% endhighlight %}\nCheers\n","permalink":"/2015/02/02/2015-02-02-legacy-md5-password-migration-symfony/","summary":"Recently I came upon a problem where the passwords of the current user base are hashed in md5 and we had to migrate them to sha. Checking around the web how others have solved this didn\u0026rsquo;t help a lot. Asking your user to login to change the password or double encode the md5 one didn\u0026rsquo;t sound like clever solutions.\nThe main authentication process in symfony is happening in DaoAuthenticationProvider.\n{% highlight php startinline=true %} if (!","title":"Migrate legacy md5 passwords to sha on symfony and fos user bundle"},{"content":"Lately I find myself in a position where I\u0026rsquo;m not sure what I want to learn next and what will be usefull for me career wise. This weekend I wanted to check out the play framework. A scala framework which supports coding in both scala and java.\nI used to work with java for many year but I wanted something else since I wasn\u0026rsquo;t enjoying the ecosystem. I will not get into details on that, java is great as language but there are a lot of applications out there which is nightmare to work with\u0026hellip;\nThe play command is replaced my an amazing tool called activator. What stunned me was the web based ide it loads out of the box to work with the code. All you need to run is \u0026ldquo;activator ui\u0026rdquo; and the following web ide loads.. ![activator ui]({{ site.url }}/assets/media/activator-ui.png)\nAs first what I\u0026rsquo;ve found quite enjoyable is that the directory and code structure is not that much different of symfony framework, on which I\u0026rsquo;m quite proficient.\nA symfony controller\u0026hellip; {% highlight php %} namespace AppBundle\\Controller;\nuse Symfony\\Component\\HttpFoundation\\Response; use Symfony\\Bundle\\FrameworkBundle\\Controller\\Controller;\nclass HelloController extends Controller { public function indexAction($name) { return new Response(\u0026rsquo;Hello \u0026lsquo;.$name.\u0026rsquo;!\u0026rsquo;); } } {% endhighlight %}\nA play controller {% highlight java %} package controllers;\nimport play.; import play.mvc.;\nimport views.html.*;\npublic class Application extends Controller {\npublic static Result index() { return ok(index.render(\u0026quot;Your new application is ready.\u0026quot;)); } } {% endhighlight %}\nTo me the actuall code is literally the same\u0026hellip;(I don\u0026rsquo;t like annotations for defining routing, I prefer separate files) The directory structure is also quite easy to get it and work with. I had to issues playing with the settings, the routes etc\u0026hellip;\nWhat really impressed be, as I mentioned at the start, is activator. Apart from web ide, as long as you start service the app it gets automatically re-compiled and loaded on every change!!! This was something that drove me to php a few years ago since hot-deploying the jar after the compilation on tomcat was a pain in the arse and tomcat needed restart after few hot deployes.\nWith activator you can also create a jar out of your app, rpm and deb packages of your application and also a super handy zip file containing all the proper dependencies and an executable to fire up you application. In theory all you need is the zip file and a reverse proxy setting on your webserver to go!!\nActivator uses sbt as build tool so I suppose a lot of those goodies comes from sbt but activator makes them really easy to handle for the developer side!!\nPlay is a super modern framework build on Akka and I would definitelly like to work with it in the near future!\nCheers\n","permalink":"/2015/01/18/2015-01-18-play-framework/","summary":"Lately I find myself in a position where I\u0026rsquo;m not sure what I want to learn next and what will be usefull for me career wise. This weekend I wanted to check out the play framework. A scala framework which supports coding in both scala and java.\nI used to work with java for many year but I wanted something else since I wasn\u0026rsquo;t enjoying the ecosystem. I will not get into details on that, java is great as language but there are a lot of applications out there which is nightmare to work with\u0026hellip;","title":"Play framework, it looks awesome!"},{"content":"Why do we have so many programming languages?\nI don\u0026rsquo;t know about you but this is a question I\u0026rsquo;m asking myself constantly when I read about new technologies and methods. My opinion is that we have so many programming languages for two basic reasons.\nThe first reason is kinda obvious and it\u0026rsquo;s called evolution. Our need to build more complex systems, our need to simplify common solutions, the software being available to more people has led us into creating new tools trying to solve those increasingly complicated problems.\nThe second reason which is something I will try to express my thoughts on, is on the same time obvious but really hard to accept and understand. The second reason is that we, as humans, think differently. Let me tell you a small story.\nI was once on a meetup here in London and as usual a fellow developer asked me what I\u0026rsquo;m doing for leaving. She is a well known and respected developer among this specific community and I have no doubt for it. Here is a snapshot of the discussion\nme: Currently I work for a fashion firm. she: And what can you possible do for a fashion firm? me: We are building the e-commerce platform from scratch writing mainly php she: Lol, do people still write php? At that point I was kinda of stunned. She actually was quite offensive in a \u0026ldquo;polite\u0026rdquo; way. First of all she made fun of the industry I\u0026rsquo;m working on, and secondly she made fun of the technology we are using. I was not mad, on the contrary, somehow she expressed this exact difference of thinking that affects a lot of arias on the software world. This is exactly the reason why we have so many programming languages, so many different software builders, so many deploy tools and so on\u0026hellip;people think differently. What is important to me is not important to you. What I find dumb you find it awesome and so on\u0026hellip;\nOur ego is forcing us to spend our energy into creating our own tools, new tools. Should we focus making the current tools smarter? I have no idea. What I do know though there is lower limit and an upper limit as well. We need tools to choose from but it shouldn\u0026rsquo;t be like drinking water from a firehose.\n","permalink":"/2014/10/05/2014-10-05-more-than-one-programming-language/","summary":"Why do we have so many programming languages?\nI don\u0026rsquo;t know about you but this is a question I\u0026rsquo;m asking myself constantly when I read about new technologies and methods. My opinion is that we have so many programming languages for two basic reasons.\nThe first reason is kinda obvious and it\u0026rsquo;s called evolution. Our need to build more complex systems, our need to simplify common solutions, the software being available to more people has led us into creating new tools trying to solve those increasingly complicated problems.","title":"More than one programming language"},{"content":"A lot of people are talking about collaboration, programming disciplines, agile methodology \u0026hellip;etc as basic principles of a modern software development team. Well \u0026hellip; bullocks.\nOne of the basic things that can make a team wither or thrive is the amount of presence of programming ego. It\u0026rsquo;s our willingness to accept a different opinion as long as it\u0026rsquo;s owner presents valid arguments to support this opinion.\nHow many times in your career haven\u0026rsquo;t you heard arguments like \u0026ldquo;just do it\u0026rdquo;, \u0026ldquo;he/she knows because he/she is good\u0026rdquo;, \u0026ldquo;it\u0026rsquo;s better because I have done it in the past\u0026rdquo; and lot more funny expressions.\nTry to listen to your colleague and open your mind to what he/she has to say. Invalid opinions are equally important to good ones just because they trigger us to think. Don\u0026rsquo;t insist on something that you know is worst than something else just because you think it will make you weak in front of your team. It\u0026rsquo;s the product you create that will be weak at the market.\nFinally, accept criticism. As long as it is polite it will only make you a better professional and it will help you grow your communication and negotiation skills.\nps: stop using the expression \u0026ldquo;I would do it like \u0026hellip;\u0026rdquo;, I want to respond \u0026ldquo;why should I care how you would do it?\u0026rdquo; \u0026hellip; instead use the power or words \u0026ldquo;I believe it is better like this because, argument 1, argument 2\u0026rdquo;. If you don\u0026rsquo;t have any arguments then say nothing.\nCheers\n","permalink":"/2014/10/01/2014-10-01-programming-ego/","summary":"A lot of people are talking about collaboration, programming disciplines, agile methodology \u0026hellip;etc as basic principles of a modern software development team. Well \u0026hellip; bullocks.\nOne of the basic things that can make a team wither or thrive is the amount of presence of programming ego. It\u0026rsquo;s our willingness to accept a different opinion as long as it\u0026rsquo;s owner presents valid arguments to support this opinion.\nHow many times in your career haven\u0026rsquo;t you heard arguments like \u0026ldquo;just do it\u0026rdquo;, \u0026ldquo;he/she knows because he/she is good\u0026rdquo;, \u0026ldquo;it\u0026rsquo;s better because I have done it in the past\u0026rdquo; and lot more funny expressions.","title":"A few thoughts on programming, and not only, ego..."},{"content":"Auto-correction/spell checking is probably one of the most used functionalities over the web.\nHas anyone thought how it is working?? How blogs, websites, that run clearly over the web, are able to serve the list nearly instantly??\nI was really curious the other day and I wanted to spend some minutes crafting a small algori\tthm to study the functionality.\nI\u0026rsquo;ve googled a bit arround for dictionaries and I\u0026rsquo;ve found (this one). It\u0026rsquo;s not perfect but enough to get me started.\nLet start\u0026hellip;my plan is to load all the words of the file to an array. Slice the array based on the first letter of the word and match somes letter based on their index.\n{% highlight php %}\n\u003c?php $handle = @fopen(\"dictionary.txt\", \"r\"); if ($handle) { while (($buffer = fgets($handle, 4096)) !== false) { $words[$buffer[0]][] = trim($buffer); } if (!feof($handle)) { echo \"Error: unexpected fgets() fail\\n\"; } fclose($handle); } ?\u003e {% endhighlight %}\nI prefer fopen because its quite faster that file_get_contents for example.\nI will get the words from the command line\u0026hellip;\n{% highlight php %}\n\u003c?php $reducedWords = array(); foreach ($argv as $input) { echo \"--- Possible corrections for $input ---\\n\"; foreach ($words[$input[0]] as $key =\u003e $value) { if (strlen($input) == strlen($value)) { $reducedWords[] = $value; } } foreach ($reducedWords as $key =\u003e $word) { $l = strlen($input); $index = 0; for ($i=0; $i \u003c $l; $i++) { if (isset($word[$i]) \u0026\u0026 $input[$i] == $word[$i]) { $index++; } } if ($index \u003e= ($l-1)) { echo \"$word\\n\"; } } } ?\u003e {% endhighlight %}\nThis algorithm is not perfect, I didn\u0026rsquo;t want to spend a lot of time on this. For example I\u0026rsquo;m checking words with only the same length like the given one which obviously is not correct for a production algorithm\u0026hellip;anyway lets see the speed with one word.\n{% highlight bash %} Argiriss-MacBook-Pro:CmdSpellCorrection argi$ php correct.php applo \u0026mdash; Possible corrections for applo \u0026mdash; apolo apple apply applz Finished in 0.89617490768433 seconds {% endhighlight %}\nHmm\u0026hellip;pretty slow I cannot say that I\u0026rsquo;m happy\u0026hellip;let\u0026rsquo;s try with 10 words\n{% highlight bash %} php correct.php ezer tost applo airplone feor shep boet garagi squirral buildang palintrame Finished in 1.7699809074402 seconds {% endhighlight %}\nInteresting\u0026hellip;I have a feeling that the I/O is a small bottleneck on the implementation\u0026hellip;let me try save the arrays per starting letter into a key/value\u0026hellip;now the full algorithm becomes\u0026hellip;\n{% highlight php %} \u0026lt;?php $memcache = new Memcache; $memcache-\u0026gt;connect(\u0026rsquo;localhost\u0026rsquo;, 11211) or die (\u0026ldquo;Could not connect\u0026rdquo;);\n$time_start = microtime(true); $words = array(); if (!is_array($memcache-\u0026gt;get('a'))) { $handle = @fopen(\u0026quot;dictionary.txt\u0026quot;, \u0026quot;r\u0026quot;); if ($handle) { while (($buffer = fgets($handle, 4096)) !== false) { $words[$buffer[0]][] = trim($buffer); } if (!feof($handle)) { echo \u0026quot;Error: unexpected fgets() fail\\n\u0026quot;; } fclose($handle); } foreach ($words as $key =\u0026gt; $eachWordArray) { $memcache-\u0026gt;set($key, $eachWordArray) or die (\u0026quot;Failed to save data at the server\u0026quot;); } } // removing the first element since it's the name of the script unset($argv[0]); $reducedWords = array(); foreach ($argv as $input) { echo \u0026quot;--- Possible corrections for $input ---\\n\u0026quot;; foreach ($memcache-\u0026gt;get($input[0]) as $key =\u0026gt; $value) { if (strlen($input) == strlen($value)) { $reducedWords[] = $value; } } foreach ($reducedWords as $key =\u0026gt; $word) { $l = strlen($input); $index = 0; for ($i=0; $i \u0026lt; $l; $i++) { if (isset($word[$i]) \u0026amp;\u0026amp; $input[$i] == $word[$i]) { $index++; } } if ($index \u0026gt;= ($l-1)) { echo \u0026quot;$word\\n\u0026quot;; } } } $time_end = microtime(true); $time = $time_end - $time_start; echo \u0026quot;Finished in $time seconds\\n\u0026quot;; {% endhighlight %}\nI expect the first run to be slow because I still have to read the file and populate memcached\u0026hellip;but the second execution\u0026hellip;\n{% highlight bash %} Argiriss-MacBook-Pro:CmdSpellCorrection argi$ php correct.php applo \u0026mdash; Possible corrections for applo \u0026mdash; apolo apple apply applz Finished in 0.076194047927856 seconds {% endhighlight %}\nAnd with 10 words..which is affected by the echo ofc\u0026hellip;\n{% highlight bash %} php correct.php ezer tost applo airplone feor shep boet garagi squirral buildang palintrame Finished in 1.0620329380035 seconds {% endhighlight %}\nPretty impressive difference!! Cache ftw!\nLike I said before this is attempt is just for the concept. The numbers can be much better with a faster machine and the actual code needs improvements, but I didn\u0026rsquo;t want to spend more than 20 minutes with this.\nHope you enjoyed it, I did :) \u0026hellip; I think I would like to implement this in Scala\u0026hellip;maybe for the next post. The code is here CmdSpellCorrection\nCheers\n","permalink":"/2014/02/12/2014-02-12-php-auto-correction-speed/","summary":"Auto-correction/spell checking is probably one of the most used functionalities over the web.\nHas anyone thought how it is working?? How blogs, websites, that run clearly over the web, are able to serve the list nearly instantly??\nI was really curious the other day and I wanted to spend some minutes crafting a small algori\tthm to study the functionality.\nI\u0026rsquo;ve googled a bit arround for dictionaries and I\u0026rsquo;ve found (this one).","title":"PHP Auto correction/spell check, speed testing"},{"content":"Hello all,\nWell, this is my first video ever and I made some language mistakes and probably the flow could have been better. This was a \u0026ldquo;one try\u0026rdquo; video and I\u0026rsquo;ve left it like this to remind me of things that I should take care for the next one.\nSo\u0026hellip;.\nA while ago when I started learning scala it would have been really helpfull for me to have a small tutorial to introduce me to the initial process of how to create a scala project.\nComing from a java background helped me move arround the tools and the process but I wanted to create a small video to help people who just want to play with the language to bootstrap an initial application.\nHere it is, even if it will help one person I will be very happy :) .\nCheers\n","permalink":"/2014/02/08/2014-02-08-first-scala-tutorial/","summary":"Hello all,\nWell, this is my first video ever and I made some language mistakes and probably the flow could have been better. This was a \u0026ldquo;one try\u0026rdquo; video and I\u0026rsquo;ve left it like this to remind me of things that I should take care for the next one.\nSo\u0026hellip;.\nA while ago when I started learning scala it would have been really helpfull for me to have a small tutorial to introduce me to the initial process of how to create a scala project.","title":"First Scala tutorial with tests"},{"content":"So looking around to improve my knowledge on how DNS works I fell into a \u0026ldquo;well hidden\u0026rdquo; chrome functionality that looks really cool!!\nChrome tries to resolve urls while the user types (DNS Prefetching) and it has an internal functionality to analyze all the relevant events.\nTry to type in the address bar\nchrome://net-internals/#dns A couple of screen shots of two tools included in the functionality\u0026hellip;\nTimeline of events\n![dns tool 1]({{ site.url }}/assets/media/dns_tool_1.png)\n\u0026ndash;\nTesting a url\n![dns tool 2]({{ site.url }}/assets/media/dns_tool_2.png)\nGo check it out\u0026hellip;\nCheers\n","permalink":"/2014/01/20/2014-01-20-chrome-dns-tools/","summary":"So looking around to improve my knowledge on how DNS works I fell into a \u0026ldquo;well hidden\u0026rdquo; chrome functionality that looks really cool!!\nChrome tries to resolve urls while the user types (DNS Prefetching) and it has an internal functionality to analyze all the relevant events.\nTry to type in the address bar\nchrome://net-internals/#dns A couple of screen shots of two tools included in the functionality\u0026hellip;\nTimeline of events\n![dns tool 1]({{ site.","title":"Chrome dns tools"},{"content":"Recently I\u0026rsquo;ve been asked how easy it will be to get the initial screenshot of a youtube video attached in an email template with as less pain as possible.\nA bit of googling told me that you can get all public information of a video using the following endpoing\nhttps://gdata.youtube.com/feeds/api/videos/videoid?v=2 By using one of my favourite songs\u0026hellip;\nhttp://www.youtube.com/watch?v=OguPPZ_7u60 The request becomes\u0026hellip;\nhttps://gdata.youtube.com/feeds/api/videos/OguPPZ_7u60?v=2 Just to test a bit whats going on I\u0026rsquo;ve tested the url with\ncurl -I https://gdata.youtube.com/feeds/api/videos/OguPPZ_7u60?v=2 Lets check the contents of the url\ncurl https://gdata.youtube.com/feeds/api/videos/OguPPZ_7u60?v=2 It\u0026rsquo;s an xml file with a lof of information\u0026hellip;lets save it in a file\ncurl https://gdata.youtube.com/feeds/api/videos/OguPPZ_7u60?v=2 \u0026gt; my_favourite_music_video_data.xml Opening the file with vim the xml is all within a single line\u0026hellip;I cannot read it like this, I need to\n:%s/\u0026gt;\u0026lt;/\u0026gt;\\r\u0026lt;/g and voi la!!!\u0026hellip;.\nI have the url that gives me a reasonable quality thumbnail of a video..\nhttps://i1.ytimg.com/vi/OguPPZ_7u60/hqdefault.jpg all that I need now is an easy way for the user to give the video code and just replace it in an img tag source that will point to the url above!! :)\nCheers\n","permalink":"/2014/01/11/2014-01-11-playing-with-the-youtube-api/","summary":"Recently I\u0026rsquo;ve been asked how easy it will be to get the initial screenshot of a youtube video attached in an email template with as less pain as possible.\nA bit of googling told me that you can get all public information of a video using the following endpoing\nhttps://gdata.youtube.com/feeds/api/videos/videoid?v=2 By using one of my favourite songs\u0026hellip;\nhttp://www.youtube.com/watch?v=OguPPZ_7u60 The request becomes\u0026hellip;\nhttps://gdata.youtube.com/feeds/api/videos/OguPPZ_7u60?v=2 Just to test a bit whats going on I\u0026rsquo;ve tested the url with","title":"Playing with the youtube api"},{"content":"Recently I fell into a quite interesting problem.\nImagine that you want to send a mass email. You have two different set of arrays. One is your mailing list and the second is your filter email list. The final recipients will be the array_intersect of those 2 arrays. So far so good\u0026hellip;\nBut what if the elements of the first array are also arrays? And what if you have stored values seperated by semicolon or any other character?\nLets split the problem into smaller ones.\nWe have the following arrays\u0026hellip;\n{% highlight php %} $array1 = array( array(\u0026rsquo;email\u0026rsquo;=\u0026gt;\u0026lsquo;test@test.com;test2@test.com\u0026rsquo;,\u0026lsquo;saluation\u0026rsquo;=\u0026gt;\u0026lsquo;Test\u0026rsquo;, \u0026lsquo;value\u0026rsquo;=\u0026gt;\u0026lsquo;some value\u0026rsquo;), array(\u0026rsquo;email\u0026rsquo;=\u0026gt;\u0026lsquo;another.test@test.com\u0026rsquo;, \u0026lsquo;salutation\u0026rsquo;=\u0026gt;\u0026lsquo;Another test\u0026rsquo;, \u0026lsquo;value\u0026rsquo;=\u0026gt;\u0026lsquo;another value\u0026rsquo;), );\n$array2 = array( array('email'=\u0026gt;'test@test.com'), array('email'=\u0026gt;'test2@test.com'), ); {% endhighlight %}\nWhat we want to achieve is an array containing the test@test.com and test2@test.com with the rest of the values.\nA simple array_intersect will not do the job, it will return a notice and the $array1 as it is. We need a different approach.\nFirst thing is to brake the element containing the semicolon into 2 different rows and copy their additional values.\n{% highlight php %} $subset = array(); foreach ($multidimentionalArray as $key =\u0026gt; $value) {\n$keys = array_keys($value); $keys = array_flip($keys); unset($keys[$filterKey]); $keys = array_flip($keys); if ( strpos($value[$filterKey], ';') !== false ) { $tmp = explode(';', $value[$filterKey]); foreach ($tmp as $tmpValue) { $subset[] = self::injectAdditionalFields($filterKey, $tmpValue, $keys, $value); } }else{ $subset[] = self::injectAdditionalFields($filterKey, $value[$filterKey], $keys, $value); } {% endhighlight %}\nWhere injectAdditionalFields is the following function.\n{% highlight php %} private static function injectAdditionalFields($filterKey, $tmpValue, $keys, $value) { $tmpTable = array(); $tmpTable[$filterKey] = $tmpValue; foreach ($keys as $key) { $tmpTable[$key] = $value[$key]; }\nreturn $tmpTable; } {% endhighlight %}\nfilterKey is the key of the internal arrays that we want to compare. In our case it will be the email. We need to append the rest of the key=\u0026gt;values to the multiple rows, based on the semicolon explosion\u0026hellip; $subset is our new array and the $tempTable is the seperation of the emails.\nAt this point the $subset table looks like this\u0026hellip;.\n{% highlight php %} Array ( [0] =\u0026gt; Array ( [email] =\u0026gt; test@test.com [saluation] =\u0026gt; Test [value] =\u0026gt; some value )\n[1] =\u0026gt; Array ( [email] =\u0026gt; test2@test.com [saluation] =\u0026gt; Test [value] =\u0026gt; some value ) [2] =\u0026gt; Array ( [email] =\u0026gt; another.test@test.com [salutation] =\u0026gt; Another test [value] =\u0026gt; another value ) ) {% endhighlight %}\nFor the final step I\u0026rsquo;ll use the array_uintersect with a custom callback function.\n{% highlight php %} $results = array_uintersect($subset, $filterArray, \u0026lsquo;self::emailCompare\u0026rsquo;);\nprivate static function emailCompare($val1, $val2) { return strcmp($val1[self::$filterKey], $val2[self::$filterKey]); } {% endhighlight %}\nAnd the result is \u0026hellip;.\n{% highlight php %} Array ( [0] =\u0026gt; Array ( [email] =\u0026gt; test@test.com [saluation] =\u0026gt; Test [value] =\u0026gt; some value )\n[1] =\u0026gt; Array ( [email] =\u0026gt; test2@test.com [saluation] =\u0026gt; Test [value] =\u0026gt; some value ) ) {% endhighlight %}\nThere you go\u0026hellip;the source code is on my github\nI\u0026rsquo;m always open to suggestions and improvements!\nCheers, Argi\n","permalink":"/2013/12/22/2013-12-22-array-intersect/","summary":"Recently I fell into a quite interesting problem.\nImagine that you want to send a mass email. You have two different set of arrays. One is your mailing list and the second is your filter email list. The final recipients will be the array_intersect of those 2 arrays. So far so good\u0026hellip;\nBut what if the elements of the first array are also arrays? And what if you have stored values seperated by semicolon or any other character?","title":"Extending array_intersect to support multidimensional arrays"},{"content":"Before I create this blog I had in my mind that bloging for software would have been extremely easy and that I would have constantly ideas to share with the community.\nI was totally wrong!! The hardest part of bloging is being unique and authentic. I see so many bloggers copying each other, so many tutorials looking \u0026ldquo;similar\u0026rdquo; and so my people doing this for the sake of blogging. I find no interest into doing tutorials that you can already google them, or writing a post sharing what I already know in order to convince a future employer of how awesome I am.\nInstead of all of this I will use this space to share techniques that saved my problem and I couldn\u0026rsquo;t find them anywhere else but also I will introduce a series of posts with the name of weekend projects.\nWeekend projects will actually be 5+5 or 2+5+3 or 2+2+2+2+2 hours of coding trying to solve a specific problem on a technology that I\u0026rsquo;m not aware of. What I want to achieve with this is to measure how easy it is to learn completely unknown things to you within a short amount of time, gathering as valuable information as possible. Let\u0026rsquo;s see how it will go\u0026hellip;\nps: yeap during the weekends I actually like resting my brain and being with friends having fun, going to the park, watching movies, exploring London etc\u0026hellip;.\nOne last thing on mad blogging\u0026hellip;.\nI was amazed the other day, in a negative way, by a job advertisement of one of the big players in the IT industry, saying that they want a world class engineer who blogs, has twitter followers, answers on forums every day and has tons of hobbies\u0026hellip;so how the hell a great engineer finds time to spend on forums and facebook\u0026hellip;why a world class engineer should prove he is one by using social media all day long?? :S\nCheers, Argi\n","permalink":"/2013/12/22/2013-12-22-weekend-project/","summary":"Before I create this blog I had in my mind that bloging for software would have been extremely easy and that I would have constantly ideas to share with the community.\nI was totally wrong!! The hardest part of bloging is being unique and authentic. I see so many bloggers copying each other, so many tutorials looking \u0026ldquo;similar\u0026rdquo; and so my people doing this for the sake of blogging. I find no interest into doing tutorials that you can already google them, or writing a post sharing what I already know in order to convince a future employer of how awesome I am.","title":"Weekend projects"},{"content":"After a looong time I decided to start \u0026ldquo;documenting\u0026rdquo; my php ideas and techniques in this personal blog.\nJust to start with something, I wanted to share with you a simple profiling technique which includes xdebug and webgrind.\nI\u0026rsquo;m using a mac and I will not get into details on how to setup php and xdebug. You can use the default php version that MacOS comes with, or handle different versions with homebrew, there are plenty of tutorials out there.\nTo start, clone webgrind from it\u0026rsquo;s github repo (Webgrind) in a folder accessible from your web server. On my machine this will be /Users/argi/Sites/php/webgrind.\nAccessing this directory from your web browser the default interface with load.\nProducing a cachegrind file using xdebug it\u0026rsquo;s really really simple.\nYou can use the command php -d xdebug.profiler_enable=On php_script.php . To make this a bit more convinient add the alias command: {% highlight bash %}alias phpp=\u0026ldquo;php -d xdebug.profiler_enable=On \u0026ldquo;{% endhighlight %} in your .bash_profile file under the home directory. Opening a new console will have the phpp command available. Just execute any script you like and it will have a file generated for this. (the debug output folder is configurable but default settings should work just fine)\n![cmd]({{ site.url }}/assets/media/phpp_command.png)\nGoing back to the browser and webgrind the generated file will be available to select it.\n![webgrind selection]({{ site.url }}/assets/media/webgrind_file_selection.png)\nPlaying a bit with the settings you can get really useful information on function calls, execution time, and many many more\u0026hellip;\n![webgrind results]({{ site.url }}/assets/media/webgrind_results.png)\nPlay with it and enjoy it :)\nArgi\n","permalink":"/2013/11/23/2013-11-23-php-script-profiling/","summary":"After a looong time I decided to start \u0026ldquo;documenting\u0026rdquo; my php ideas and techniques in this personal blog.\nJust to start with something, I wanted to share with you a simple profiling technique which includes xdebug and webgrind.\nI\u0026rsquo;m using a mac and I will not get into details on how to setup php and xdebug. You can use the default php version that MacOS comes with, or handle different versions with homebrew, there are plenty of tutorials out there.","title":"Profiling php scripts"}]